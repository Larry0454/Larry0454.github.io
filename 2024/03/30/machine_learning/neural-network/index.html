

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="WangLe">
  <meta name="keywords" content="">
  
    <meta name="description" content="神经网络 一、感知器  感知器的定义：以一个实值向量为输入，计算输入中各分量的线性组合，最后通过激活函数计算输出 感知器的输出计算公式： \[ o(x_1, ..., x_n) &#x3D; \begin{cases} 1 &amp; \omega_0 + \omega_1 x_1 + ... + \omega_n x_n \gt 0 \\ -1 &amp; otherwise \end{">
<meta property="og:type" content="article">
<meta property="og:title" content="『machine learning-7』neural network">
<meta property="og:url" content="http://larry0454.github.io/2024/03/30/machine_learning/neural-network/index.html">
<meta property="og:site_name" content="Blog">
<meta property="og:description" content="神经网络 一、感知器  感知器的定义：以一个实值向量为输入，计算输入中各分量的线性组合，最后通过激活函数计算输出 感知器的输出计算公式： \[ o(x_1, ..., x_n) &#x3D; \begin{cases} 1 &amp; \omega_0 + \omega_1 x_1 + ... + \omega_n x_n \gt 0 \\ -1 &amp; otherwise \end{">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://larry0454.github.io/2024/03/30/machine_learning/neural-network/BP%E7%AE%97%E6%B3%95.png">
<meta property="og:image" content="http://larry0454.github.io/2024/03/30/machine_learning/neural-network/Elman%E7%BD%91%E7%BB%9C.png">
<meta property="article:published_time" content="2024-03-30T00:29:00.000Z">
<meta property="article:modified_time" content="2024-07-22T05:32:53.837Z">
<meta property="article:author" content="WangLe">
<meta property="article:tag" content="machine learning">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://larry0454.github.io/2024/03/30/machine_learning/neural-network/BP%E7%AE%97%E6%B3%95.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>『machine learning-7』neural network - Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"larry0454.github.io","root":"/","version":"1.9.4","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archives</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tags</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/cover.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="『machine learning-7』neural network"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-03-30 08:29" pubdate>
          March 30, 2024 am
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          6.8k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          57 mins
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> views
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">『machine learning-7』neural network</h1>
            
            
              <div class="markdown-body">
                
                <h2 id="神经网络">神经网络</h2>
<h3 id="一感知器">一、感知器</h3>
<ul>
<li><p>感知器的定义：以一个<strong>实值向量</strong>为输入，计算输入中<strong>各分量的线性组合</strong>，最后通过<strong>激活函数</strong>计算输出</p>
<p>感知器的输出计算公式： <span class="math display">\[
o(x_1, ..., x_n) =
\begin{cases}
1 &amp; \omega_0 + \omega_1 x_1 + ... + \omega_n x_n \gt 0 \\
-1 &amp; otherwise
\end{cases}
\]</span> 上式中激活函数取符号函数<span class="math inline">\(sgn(x)\)</span></p>
<p><strong>注意</strong>：一般会设置一个<strong>固定的输入<span class="math inline">\(x_0 = 1\)</span></strong>，则上面的不等式可写成
<span class="math inline">\(\sum_{i=0}^n \omega_i x_i \gt 0\)</span></p>
<hr></li>
<li><p>感知器的表征能力：可解决<strong>线性可分问题</strong></p>
<ul>
<li><p>线性可分：设样例是n维实例空间中的点</p>
<p>若存在<u>一个超平面决策面</u>正好可以<strong>将正反样例划分开</strong>，说明被划分的集合称为线性可分集合</p>
<p>超平面方程即为 <span class="math inline">\(\vec{\omega} \cdot \vec{x}
= 0\)</span></p></li>
<li><p>m-of-n
问题：设1为布尔真、0为布尔假；要使得感知器输出为真，则感知器的<u>n个输入中至少需有m个输入为真</u></p>
<p>设置所有的 <span class="math inline">\(\omega_i\)</span> = 0.5（<span class="math inline">\(i \ge\)</span> 1），再设置适当的阈值（<span class="math inline">\(\omega_0\)</span>）即可；如“或门”的<span class="math inline">\(\omega_0 \gt -0.5\)</span></p>
<p><strong>注意</strong>：“与或非”都是常见的线性可分运算，“异或”不是（不能用单一的感知器计算）</p></li>
</ul>
<hr></li>
<li><p>感知器的训练法则：从随机的一组权值开始，将所有训练样例提交给感知器</p>
<p>只要出现误分类就修改权值，直至感知器<strong>正确分类所有样例</strong>，设输入样例<span class="math inline">\((x_i, t)\)</span>对应的权值为<span class="math inline">\(\omega_i\)</span>： <span class="math display">\[
\begin{align}
\omega_i &amp;\leftarrow \omega_i + \Delta \omega_i \\
其中权重增量 \Delta \omega_i &amp;= \eta (t - o)x_i
\end{align}
\]</span> 其中o表示感知器的当前输出，<span class="math inline">\(\eta
\in (0, 1)\)</span>表示<strong>学习速率</strong></p>
<p>由训练法则可知，若<u>当前输出</u>与<u>样例属性值</u>相等，对应权值就不会调整</p>
<hr></li>
<li><p>梯度下降和delta法则：</p>
<ul>
<li><p>感知器训练法则：仅能对线性可分的样本做分类</p></li>
<li><p>delta法则：通过<strong>梯度下降</strong>求出非线性可分计算的最佳近似（<u>学习率
* 输入值 * 输出误差</u>）</p></li>
<li><p>梯度下降：搜索使<strong>误差最小化</strong>的权值向量，设线性单元输出<span class="math inline">\(o(\vec{x}) = \vec{\omega} \cdot
\vec{x}\)</span></p>
<ul>
<li><p>训练误差：<span class="math inline">\(E(\vec{\omega}) =
\frac{1}{2} \sum_{d \in D} (t_d - o_d)^2\)</span></p>
<p>其中D是样本集，<span class="math inline">\(t_d\)</span>是样本属性值，<span class="math inline">\(o_d\)</span>是当前线性单元的输出</p></li>
<li><p>梯度：函数E上升<strong>最陡峭</strong>的方向，<span class="math inline">\(\nabla E = (\frac{\partial E}{\partial \omega_0},
...,\frac{\partial E}{\partial \omega_n})\)</span>，故设调整方向 <span class="math inline">\(\Delta \vec{\omega} = -\eta \nabla
E(\vec{\omega})\)</span></p></li>
<li><p>梯度下降算法：初始化<span class="math inline">\(\Delta \omega_i =
0\)</span>，对训练样本中的所有样本<span class="math inline">\((\vec{x},
t)\)</span>，<u>累计</u>计算所有的<strong>权值增量</strong>： <span class="math display">\[
\Delta \omega_i \leftarrow \Delta \omega_i + \eta (t - o) x_i
\]</span> 最后<strong>调整各权值分量</strong>： <span class="math display">\[
\omega_i \leftarrow \omega_i + \Delta \omega_i
\]</span> <strong>注意</strong>：学习速率<span class="math inline">\(\eta\)</span>必须<strong>足够小</strong>，以防止步幅过大跨过最小误差对应的权值向量</p></li>
</ul></li>
<li><p>随机梯度下降：解决<u>收敛速度过慢</u>或<u>陷入局部极小</u>的问题</p>
<ul>
<li>标准梯度下降：在权值更新之前<strong>先汇总</strong>所有样例的误差
<span class="math display">\[
\Delta \omega_i = \eta \sum_{d \in D} (t_d - o_d)x_{di}
\]</span> <u>随机梯度下降</u>：根据每个样例<span class="math inline">\((\vec{x},
t)\)</span><strong>直接更新</strong>各权值 <span class="math display">\[
\Delta \omega_i = \eta (t - o) x_i
\]</span></li>
</ul></li>
</ul>
<p><strong>注意</strong>：感知器训练法则是根据<strong>阈值输出</strong>调整权值，而delta法则是直接根据<strong>线性单元输出</strong>调整权值</p></li>
</ul>
<hr>
<h3 id="二多层网络和反向传播算法">二、多层网络和反向传播算法 ⭐</h3>
<ul>
<li><p>多层网络：具有<strong>隐藏层</strong>，能够表示<u>高度非线性</u>的决策面（如曲面）</p></li>
<li><p>可微阈值单元：适用于梯度下降算法<strong>非线性</strong>的<strong>可微</strong>函数</p>
<p>sigmoid函数：常见的平滑可微的阈值函数 <span class="math display">\[
sigmoid(x) = \frac{1}{1 + e^{-x}}
\]</span></p></li>
<li><p>sigmoid函数的特殊性质：记sigmoid为<span class="math inline">\(\sigma\)</span>，有<span class="math inline">\(\sigma&#39; = \sigma(1-\sigma)\)</span></p>
<hr></li>
<li><p>反向传播算法（BP算法）：通过<strong>梯度下降</strong>最小化<u>网络输出值</u>和<u>目标值之间</u>的误差平方</p>
<p>设<span class="math inline">\(x_{ji}\)</span>为神经元j的第i个<strong>输入</strong>，<span class="math inline">\(\omega_{ji}\)</span>表示对应的<strong>权重</strong>，<span class="math inline">\(net_j\)</span>表示神经元j<strong>输入的线性加权</strong>，<span class="math inline">\(o_j\)</span>表示神经元j的<strong>阈值输出</strong></p>
<p>downstream(j)表示以神经元j的输出为直接输入的<strong>下一层神经元</strong>的集合</p>
<p><img src="/2024/03/30/machine_learning/neural-network/BP算法.png" srcset="/img/loading.gif" lazyload width="80%" height="80%">
对于训练样本集中的每个样例<span class="math inline">\((\vec{x_i},
t)\)</span>，将其输入网络，使误差沿网络反向传播：</p>
<ol type="1">
<li><p>对于网络中的每个<strong>输出神经元k</strong>，计算其误差项<span class="math inline">\(\delta_k = o_k(1-o_k)(t_k - o_k)\)</span> （<span class="math inline">\(\sigma&#39; \Delta out）\)</span></p></li>
<li><p>对于网络中的每个<strong>隐层神经元h</strong>，计算其误差项<span class="math inline">\(\delta_h = o_h(1-o_h) \sum_{k \in downstream(h)}
\delta_k \omega_{kh}\)</span>（<u><strong>向前递推</strong></u>）</p>
<p>第m层神经元的误差项是由<strong>所有第m+1层神经元</strong>的误差项推导出来的</p></li>
<li><p><strong>更新</strong>各网络权值<span class="math inline">\(\omega_{ji} \leftarrow \omega_{ji} + \Delta
\omega_{ji}\)</span>，其中<span class="math inline">\(\Delta \omega_{ji}
= \eta \delta_{j} x_{ji}\)</span></p></li>
</ol>
<p>反向传播算法的终止条件：固定的迭代次数（一轮<u>迭代</u>指<u>读取一遍训练集</u>），或误差降至某个阈值以下</p></li>
</ul>
<p>​
<strong>注意</strong>：上面的BP算法适用于任何“无环”网络学习，downstream也不要求各层网络中的神经元整齐排列在一行内</p>
<ul>
<li><p>BP算法的推导思路：设样本d的误差平方<span class="math inline">\(E_d(\vec{\omega}) = \frac{1}{2} \sum_{k \in
outputs} (t_k - o_k)^2\)</span></p>
<p>由梯度下降算法，对每个训练样例d，需要求解 <span class="math inline">\(\Delta \omega_{ji} = -\eta \frac{\partial
E_d}{\partial \omega_{ji}}\)</span>，即调整梯度的增量</p>
<p>神经元j的输入权值<span class="math inline">\(\omega_{ji}\)</span><u>仅通过加权和<span class="math inline">\(net_j\)</span></u>影响<strong>后方的网络</strong>，由链式法则：
<span class="math display">\[
\begin{align}
\frac{\partial E_d}{\partial \omega_{ji}} &amp;= \frac{\partial
E_d}{\partial net_j} \frac{\partial net_j}{\partial \omega_{ji}} \\
&amp;= \frac{\partial E_d}{\partial net_j} x_{ji} = -\delta_j x_{ji}
\end{align}
\]</span> 若神经元j是<strong>输出神经元</strong>：神经元j的加权和<span class="math inline">\(net_j\)</span><u>仅通过输出<span class="math inline">\(o_j\)</span></u>影响<strong>后方的网络</strong>
<span class="math display">\[
\begin{align}
\frac{\partial E_d}{\partial net_j} &amp;= \frac{\partial E_d}{\partial
o_j} \frac{\partial o_j}{\partial net_j} \\
&amp;= \frac{\partial}{\partial o_j}[\frac{1}{2} (t_j - o_j)^2]
\frac{\partial \sigma(net_j)}{\partial net_j} \\
&amp;= -(t_j - o_j)o_j(1-o_j)
\end{align}
\]</span> 若神经元j是<strong>隐层神经元</strong>：神经元j的加权和<span class="math inline">\(net_j\)</span><u>通过其<strong>所有</strong>下游神经元</u>影响<strong>后方的网络</strong>
<span class="math display">\[
\begin{align}
\frac{\partial E_d}{\partial net_j} &amp;= \sum_{k \in downstream(j)}
\frac{\partial E_d}{\partial net_k} \frac{\partial net_k}{\partial
net_j} \\
&amp;= \sum_{k \in downstream(j)} -\delta_k (\frac{\partial
net_k}{\partial o_j} \frac{\partial o_j}{\partial net_j}) \\
&amp;= \sum_{k \in downstream(j)} -\delta_k \omega_{kj} o_j(1-o_j) \\
&amp;= -\delta_j
\end{align}
\]</span></p>
<hr></li>
<li><p>收敛性与局部极小值</p>
<ul>
<li><p>基于迭代的反向传播算法可能导致误差E收敛到某个<strong>局部极小值</strong></p></li>
<li><p>如何缓解局部极小值问题：“启发式规则”</p>
<ul>
<li><p>为梯度增量增设一个<strong>冲量项</strong>：<span class="math inline">\(\Delta \omega_{ji}(n) = \eta \delta_jx_{ji} +
\alpha \Delta \omega_{ji}(n - 1)\)</span></p>
<p>其中增加的第二项就是冲量项，冲量常数 <span class="math inline">\(0
\le \alpha \lt 1\)</span>；n表示第n轮迭代</p>
<p>冲量项有可能助力网络跳出局部极小值</p></li>
<li><p>以<strong>多组不同初始权重</strong>训练神经网络，取其中误差E最小的参数训练结果作为最终参数（相当于从<u>多个初始点</u>开始搜索）</p></li>
<li><p>使用“模拟退火”技术：模拟退火的每一步迭代都以<strong>一定概率</strong>接受比当前解更差的结果，从而有概率跳出局部极小值</p></li>
</ul></li>
</ul>
<hr></li>
<li><p>前馈网络的表征能力：</p>
<ul>
<li><p>布尔函数：任何布尔函数都可由<strong>两层网络</strong>准确表示</p>
<p>对<u>每种可能的输入向量</u>，创建不同的隐藏神经元，设置其权值使得<strong>仅当对应的向量输入才能激活它</strong></p>
<p>再将输出单元设置为一个<strong>仅可由期待的输入向量</strong>激活的<strong>或门</strong></p></li>
<li><p>连续函数：每个有界的连续函数可由<strong>两层网络</strong>以<u>任意小的误差</u>逼近；隐藏层使用sigmoid函数，输出层使用（非阈值）线性单元</p></li>
<li><p>任意函数：任意函数可被<strong>三层网络</strong>以<u>任意精度</u>逼近；两个隐藏层使用sigmoid函数，输出层使用（非阈值）线性单元</p></li>
</ul></li>
</ul>
<hr>
<h3 id="三关于神经网络的拓展讨论">三、关于神经网络的拓展讨论</h3>
<ul>
<li><p>误差函数的选取：除了误差平方和函数E，还有其它目标函数可供选择</p>
<ul>
<li><p>为权值增加一个<strong>惩罚项</strong>：通过<u>权重的平方和</u>考察网络的<strong>复杂程度</strong>，减小<strong>过拟合</strong>的风险，<span class="math inline">\(\gamma \in (0, 1)\)</span> <span class="math display">\[
E(\vec{\omega}) = \frac{1}{2} \sum_{d \in D} \sum_{k \in outputs}
(t_{kd} - o_{kd})^2 + \gamma \sum_{i, j} \omega_{ji}^2
\]</span></p></li>
<li><p>对误差增加一项<strong>目标函数的斜率或导数</strong>：考察<u>训练导数</u>和<u>网络的实际导数</u>间的差异
<span class="math display">\[
E(\vec{\omega}) = \frac{1}{2} \sum_{d \in D} \sum_{k \in outputs}
[(t_{kd} - o_{kd})^2 + \mu \sum_{j \in inputs}(\frac{\partial
t_{kd}}{\partial x_d^j} - \frac{\partial o_{kd}}{\partial x_d^j})^2]
\]</span> 其中<span class="math inline">\(x_d^j\)</span>表示训练实例d的第j个输入单元的值</p></li>
<li><p>设置目标值为<strong>交叉熵</strong>：将<strong>离散的分类输出</strong>转化为<strong>连续的概率输出</strong>，交叉熵定义：
<span class="math display">\[
-\sum_{d \in D} t_d\log o_d + (1 - t_d)\log(1 - o_d)
\]</span> 其中<span class="math inline">\(o_d\)</span>表示对样例d输出的<strong>概率估计</strong>，<span class="math inline">\(t_d\)</span>表示训练样例的<strong>目标值</strong></p></li>
</ul>
<hr></li>
<li><p>其它常见神经网络：</p>
<ul>
<li><p>RBF网络：单隐层前馈神经网络</p>
<p>其激活函数：径向基函数，表示样本到数据中心的欧氏距离 <span class="math display">\[
\rho(x, c_i) = e^{-\beta_i |{|{x - c_i}}|| ^2}
\]</span> 输出层是隐层神经元输出的线性组合：其中q表示q个输出神经元 <span class="math display">\[
\Phi(x) = \sum_{i=1}^q \omega_i \rho(x, c_i)
\]</span></p>
<hr></li>
<li><p>ART网络：自适应谐振理论网络，<strong>竞争型学习</strong>的代表</p>
<ul>
<li><p>网络组成：比较层、识别层、<strong>识别阈值</strong>、重置模块</p>
<ul>
<li><p>比较层：<strong>接收</strong>输入样本，并将其<strong>传递</strong>给识别层神经元</p></li>
<li><p>识别层：该层的各神经元都拥有其对应模式的<strong>代表向量</strong>，与输入向量距离最近的神经元<strong>获胜</strong>，并抑制其它神经元的激活</p>
<ul>
<li>输入向量和获胜单元间距<strong>大于识别阈值</strong>：该输入样本被<strong>归为</strong>代表向量所属类，并<strong>更新连接权</strong>，使得下次接收到<u>类似向量</u>后会计算出<strong>更大的相似度</strong></li>
<li>输入向量和获胜单元间距<strong>不大于识别阈值</strong>：重置模块在识别层设置<strong>新的神经元</strong>，<u>其代表向量即为当前输入向量</u></li>
</ul>
<p><strong>注意</strong>：如果识别阈值较高，就会划分出<strong>更多更精细</strong>的代表向量</p></li>
</ul></li>
<li><p>“可塑性”和“稳定性”：</p>
<ul>
<li><p>“可塑性”指神经网络要拥有<u>学习新知识</u>的能力</p></li>
<li><p>“稳定性”指神经网络在学习新知识的<u>同时要保持对旧知识的记忆</u></p></li>
</ul></li>
</ul>
<hr></li>
<li><p>SOM网络：自组织映射网络，竞争型学习的代表</p>
<ul>
<li><p>特点：可将高维输入映射到低维（如二维）空间</p>
<p>同时保证高维空间中<u>相似的样本点</u>映射到输出层中的<u>邻近神经元</u></p></li>
<li><p>输出层神经元：每个神经元都有一个<strong>权向量</strong>，<u>与输入向量间距最近</u>的神经元称为<strong>最佳匹配单元</strong></p>
<p>最佳单元邻近的权向量会被调整，使其<strong>周围权向量与输入向量的间距缩小</strong></p></li>
</ul>
<hr></li>
<li><p>级联相关网络：结构自适应网络的重要代表</p>
<ul>
<li><p>特点：在训练过程中寻找最符合数据特点的<strong>网络结构</strong></p></li>
<li><p>级联：网络层次连接的层次结构，在训练的过程中可能会有<strong>新的隐层神经元加入</strong></p></li>
<li><p>相关：<strong>最大化</strong><u>新神经元的输出</u>与<u>网络误差</u>间的相关性</p>
<p><strong>注意</strong>：新神经元的<u>输入端权值是固定的</u>，其输出端权重是训练可变的对象</p></li>
</ul>
<hr></li>
<li><p>Elman网络：递归神经网络的代表</p>
<ul>
<li><p>递归神经网络：包含<strong>有向环</strong>结构，神经元的输出可反馈作为输入信号，可处理<strong>时序相关</strong>的变化</p>
<figure style="text-align:center;">
<p><img src="/2024/03/30/machine_learning/neural-network/Elman网络.png" srcset="/img/loading.gif" lazyload width="30%"></p>
<figcaption>
<p>Elman递归网络结构</p>
</figcaption>
</figure></li>
</ul>
<hr></li>
<li><p>Boltzmann机：基于能量的模型</p>
<ul>
<li><p>结构：由布尔型神经元组成（抑制“0”或激活“1”）；包含“显层”和“隐层”神经元，两层神经元间形成<u>全连接</u></p></li>
<li><p>状态向量<span class="math inline">\(\vec{s}\)</span>：<span class="math inline">\(\vec{s} \in \{0, 1\}^n\)</span>
表示n个神经元的状态</p></li>
<li><p>Boltzmann机能量：设<span class="math inline">\(\omega_{ij}\)</span>表示神经元i和j之间权连接，<span class="math inline">\(\theta_i\)</span>表示神经元i的阈值，n神经元能量：
<span class="math display">\[
E(\vec{s}) = -\sum_{i=1}^{n-1} \sum_{j=i+1}^n \omega_{ij} s_i s_j -
\sum_{i=1}^n \theta_i s_i
\]</span> 即<u>总能量 = 边权能量 + 节点能量</u></p>
<p>状态向量<span class="math inline">\(\vec{s}\)</span><strong>出现的概率</strong>仅有其能量和所有可能的状态向量的能量决定：
<span class="math display">\[
P(\vec{s}) = \frac{e^{-E(\vec{s})}}{\sum_{\vec{t}}e^{-E(\vec{t})}}
\]</span></p></li>
<li><p>训练过程：将每个<u>样本</u>视为一个<u>状态向量</u>，<strong>最大化其出现的概率</strong></p></li>
<li><p>CD算法：设网络中有d个<u>显层</u>神经元和q个<u>隐层</u>神经元</p>
<p>令<span class="math inline">\(\vec{v}\)</span>和<span class="math inline">\(\vec{h}\)</span>分别表示显层与隐层的<strong>状态向量</strong>，现根据样本<span class="math inline">\(\vec{v}\)</span>计算隐层概率分布： <span class="math display">\[
P(\vec{h}|\vec{v}) = \Pi_{i=1}^q P(h_i | \vec{v})
\]</span> 再根据上述概率分布<strong>采样</strong>得到隐层向量<span class="math inline">\(\vec{h}\)</span>，并<strong>计算</strong>显层概率分布：
<span class="math display">\[
P(\vec{v}|\vec{h}) = \Pi_{i=1}^d P(v_i | \vec{h})
\]</span> 由上述概率分布<strong>采样</strong>得到<span class="math inline">\(\vec{v}&#39;\)</span>，类似地由<span class="math inline">\(\vec{v}&#39;\)</span><strong>计算</strong>获得<span class="math inline">\(\vec{h}&#39;\)</span>，最后<strong>更新连接权重</strong>：
<span class="math display">\[
\Delta \omega = \eta (\vec{v} \vec{h}^T - \vec{v}&#39; \vec{h}&#39;^T)
\]</span></p></li>
</ul></li>
</ul></li>
</ul>
<hr>
<ul>
<li><p>深度学习：一般指<strong>很深层</strong>的神经网络</p>
<ul>
<li><p>无监督逐层训练：“预训练 + 微调”：</p>
<ul>
<li>预训练：每次训练<strong>一层</strong>隐节点：
<ol type="1">
<li>将上一层隐节点的输出作为输入</li>
<li>将本层隐节点的输出作为下一层隐节点的输入</li>
</ol></li>
<li>微调：预训练全部完成后，对整个网络进行微调（如BP算法）</li>
</ul>
<p>这种训练方式实际上是从各层<strong>局部最优</strong>出发寻找<strong>全局最优</strong></p>
<hr></li>
<li><p>“权共享”策略：让一组神经元共享<u>相同的连接权</u>，在<strong>卷积神经网络</strong>中应用较多</p>
<ul>
<li><p>卷积层：包含多个<strong>特征映射</strong>，即神经元组成的平面</p></li>
<li><p>特征映射：其中的<u>每个神经元</u>负责从<strong>输入的部分区域</strong>内通过卷积滤波器<strong>提取特征</strong></p></li>
<li><p>采样层：其中的<u>每个神经元</u>负责从<strong>特征映射层的部分区域</strong>计算输出，起到<strong>精简数据量</strong>的作用</p>
<p>通过复合卷积层 +
采样层，最终实现了将<u>高维信息</u>映射到<u>低维数据</u>的xiao'guo</p></li>
</ul>
<p><strong>注意</strong>：层数越多，特征越全局</p></li>
<li><p>池化：对不同位置的特征值进行聚合统计（max or min or
random），达到降维 + 防止过拟合的效果</p></li>
<li><p>梯度消失：随着隐藏层数量的增加，不同层之间的梯度开始消失，下面是常见的解决方案：</p>
<ul>
<li>线性整流激活函数：ReLU(x) = max(0, x)</li>
<li>归一化：对激活后的输出进行归一化</li>
<li>残差网络：在输入和输出之间添加跳跃连结，允许梯度直接流过</li>
</ul></li>
</ul></li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/machine-learning/">#machine learning</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>『machine learning-7』neural network</div>
      <div>http://larry0454.github.io/2024/03/30/machine_learning/neural-network/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>WangLe</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>March 30, 2024</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/04/04/computer_network/network-layer/" title="『computer network-3』network layer">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">『computer network-3』network layer</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/03/28/linux/view-from-shell/" title="『linux-1』view from shell">
                        <span class="hidden-mobile">『linux-1』view from shell</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
